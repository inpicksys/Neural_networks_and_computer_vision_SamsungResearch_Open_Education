{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Find the activation that causes the lowest gradient attenuation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaeqQ5rdkQL6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "7cdf9fa2-912e-441e-f635-2337d0039c18"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "seed = int(11)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "NUMBER_OF_EXPERIMENTS = 200\n",
        "\n",
        "class SimpleNet(torch.nn.Module):\n",
        "    def __init__(self, activation):\n",
        "        super().__init__()\n",
        "\n",
        "        self.activation = activation\n",
        "        self.fc1 = torch.nn.Linear(1, 1, bias=False)  # one neuron without bias\n",
        "        self.fc1.weight.data.fill_(1.)  # init weight with 1\n",
        "        self.fc2 = torch.nn.Linear(1, 1, bias=False)\n",
        "        self.fc2.weight.data.fill_(1.)\n",
        "        self.fc3 = torch.nn.Linear(1, 1, bias=False)\n",
        "        self.fc3.weight.data.fill_(1.)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.activation(self.fc1(x))\n",
        "        x = self.activation(self.fc2(x))\n",
        "        x = self.activation(self.fc3(x))\n",
        "        return x\n",
        "\n",
        "    def get_fc1_grad_abs_value(self):\n",
        "        return torch.abs(self.fc1.weight.grad)\n",
        "\n",
        "def get_fc1_grad_abs_value(net, x):\n",
        "    output = net.forward(x)\n",
        "    output.backward()  # no loss function. Pretending that we want to minimize output\n",
        "                       # In our case output is scalar, so we can calculate backward\n",
        "    fc1_grad = net.get_fc1_grad_abs_value().item()\n",
        "    net.zero_grad()\n",
        "    return fc1_grad\n",
        "\n",
        "from torch.nn import *\n",
        "\n",
        "activs = [ELU, Hardtanh, LeakyReLU, LogSigmoid, PReLU, ReLU, ReLU6, RReLU, SELU, CELU, Sigmoid, Softplus, Softshrink, Softsign, Tanh, Tanhshrink, Hardshrink]\n",
        "\n",
        "for i in activs:\n",
        "    activation =  i() # Try different activations to get biggest gradient\n",
        "              # ex.: torch.nn.Tanh()\n",
        "\n",
        "    net = SimpleNet(activation=activation)\n",
        "\n",
        "    fc1_grads = []\n",
        "    for x in torch.randn((NUMBER_OF_EXPERIMENTS, 1)):\n",
        "        fc1_grads.append(get_fc1_grad_abs_value(net, x))\n",
        "\n",
        "# Проверка осуществляется автоматически, вызовом функции:\n",
        "    print(i, np.mean(fc1_grads))\n",
        "# (раскомментируйте, если решаете задачу локально)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torch.nn.modules.activation.ELU'> 0.46825177246239036\n",
            "<class 'torch.nn.modules.activation.Hardtanh'> 0.2953543031180743\n",
            "<class 'torch.nn.modules.activation.LeakyReLU'> 0.3869488415962411\n",
            "<class 'torch.nn.modules.activation.LogSigmoid'> 0.2267814244981855\n",
            "<class 'torch.nn.modules.activation.PReLU'> 0.35629074233864233\n",
            "<class 'torch.nn.modules.activation.ReLU'> 0.39361816555727275\n",
            "<class 'torch.nn.modules.activation.ReLU6'> 0.3758565194578841\n",
            "<class 'torch.nn.modules.activation.RReLU'> 0.3941042095749435\n",
            "<class 'torch.nn.modules.activation.SELU'> 0.5512814357271418\n",
            "<class 'torch.nn.modules.activation.CELU'> 0.4496136348252185\n",
            "<class 'torch.nn.modules.activation.Sigmoid'> 0.00696174914824951\n",
            "<class 'torch.nn.modules.activation.Softplus'> 0.2403151054587215\n",
            "<class 'torch.nn.modules.activation.Softshrink'> 0.24748901307582855\n",
            "<class 'torch.nn.modules.activation.Softsign'> 0.06543945850222371\n",
            "<class 'torch.nn.modules.activation.Tanh'> 0.16056774482713082\n",
            "<class 'torch.nn.modules.activation.Tanhshrink'> 0.024549292848711\n",
            "<class 'torch.nn.modules.activation.Hardshrink'> 0.7263762906193734\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}